{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatasetFile='../../../data/REALoc/S1_dataset/5939pdataset/idLocation.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDatasetFile='../../../data/REALoc/S1_dataset/920pdataset/test_idLocation_code.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset=pd.read_csv(testDatasetFile,sep='|',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset=pd.read_csv(trainDatasetFile,sep='|',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P00505</td>\n",
       "      <td>1 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q9NP58</td>\n",
       "      <td>1 3 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q96HD9</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O43687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q8N7J2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1\n",
       "0  P00505    1 4 \n",
       "1  Q9NP58  1 3 4 \n",
       "2  Q96HD9    1 2 \n",
       "3  O43687      1 \n",
       "4  Q8N7J2      1 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O95866</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q70Z44</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q5I7T1</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O14514</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q9H159</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0               1\n",
       "0  O95866  Cell_membrane \n",
       "1  Q70Z44  Cell_membrane \n",
       "2  Q5I7T1  Cell_membrane \n",
       "3  O14514  Cell_membrane \n",
       "4  Q9H159  Cell_membrane "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre process lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLables=trainDataset[[1]]\n",
    "multiplexProteinsIndicesTrain=[]\n",
    "trainLablesOneHot=np.zeros(shape=(trainLables.shape[0],6))\n",
    "\n",
    "lableDict={'Cell_membrane':0,'Cytoplasm':1,'ER_Golgi':2,'Mitochondrion':3,'Nucleus':4,'Secreted':5}\n",
    "\n",
    "for i,row in trainLables.iterrows():\n",
    "    lables=row.str.split(' ')\n",
    "    lables=list(lables)\n",
    "    lables=lables[0]\n",
    "    lables=lables[:-1]\n",
    "    if len(lables)> 1:\n",
    "        multiplexProteinsIndicesTrain.append(i)\n",
    "    for lable in lables:\n",
    "        trainLablesOneHot[i,lableDict[lable]]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5939, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLablesOneHot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLables=testDataset[[1]]\n",
    "multiplexProteinsIndices=[]\n",
    "testLablesOneHot=np.zeros(shape=(testLables.shape[0],6))\n",
    "\n",
    "\n",
    "for i,row in testLables.iterrows():\n",
    "    lables=row.str.split(' ')\n",
    "    lables=list(lables)\n",
    "    lables=lables[0]\n",
    "    lables=lables[:-1]\n",
    "    if len(lables)> 1:\n",
    "        multiplexProteinsIndices.append(i)\n",
    "    for lable in lables:\n",
    "        testLablesOneHot[i,int(lable)-1]=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLablesOneHot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map uniprot to String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* STring IDs are required for embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from  utils import UniprotID_to_StringId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Train IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Matched:  140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:61: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "dfTrain=trainDataset[[0]]\n",
    "\n",
    "dfTrain.columns=['uniprot_ac']\n",
    "dfTrain=UniprotID_to_StringId(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_ac</th>\n",
       "      <th>uniprot_ac_uniprot_id</th>\n",
       "      <th>string_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O95866</td>\n",
       "      <td>O95866|G6B_HUMAN</td>\n",
       "      <td>9606.ENSP00000364964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q70Z44</td>\n",
       "      <td>Q70Z44|5HT3D_HUMAN</td>\n",
       "      <td>9606.ENSP00000371929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q5I7T1</td>\n",
       "      <td>Q5I7T1|AG10B_HUMAN</td>\n",
       "      <td>9606.ENSP00000310120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O14514</td>\n",
       "      <td>O14514|AGRB1_HUMAN</td>\n",
       "      <td>9606.ENSP00000430945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q9H159</td>\n",
       "      <td>Q9H159|CAD19_HUMAN</td>\n",
       "      <td>9606.ENSP00000262150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot_ac uniprot_ac_uniprot_id             string_id\n",
       "0     O95866      O95866|G6B_HUMAN  9606.ENSP00000364964\n",
       "1     Q70Z44    Q70Z44|5HT3D_HUMAN  9606.ENSP00000371929\n",
       "2     Q5I7T1    Q5I7T1|AG10B_HUMAN  9606.ENSP00000310120\n",
       "3     O14514    O14514|AGRB1_HUMAN  9606.ENSP00000430945\n",
       "4     Q9H159    Q9H159|CAD19_HUMAN  9606.ENSP00000262150"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Test IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Matched:  18\n"
     ]
    }
   ],
   "source": [
    "dfTest=testDataset[[0]]\n",
    "\n",
    "dfTest.columns=['uniprot_ac']\n",
    "\n",
    "dfTest=UniprotID_to_StringId(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TripletProt Network and generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  utils import generate_tripletProt_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Train Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_ac</th>\n",
       "      <th>uniprot_ac_uniprot_id</th>\n",
       "      <th>string_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O95866</td>\n",
       "      <td>O95866|G6B_HUMAN</td>\n",
       "      <td>9606.ENSP00000364964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q70Z44</td>\n",
       "      <td>Q70Z44|5HT3D_HUMAN</td>\n",
       "      <td>9606.ENSP00000371929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q5I7T1</td>\n",
       "      <td>Q5I7T1|AG10B_HUMAN</td>\n",
       "      <td>9606.ENSP00000310120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O14514</td>\n",
       "      <td>O14514|AGRB1_HUMAN</td>\n",
       "      <td>9606.ENSP00000430945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q9H159</td>\n",
       "      <td>Q9H159|CAD19_HUMAN</td>\n",
       "      <td>9606.ENSP00000262150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot_ac uniprot_ac_uniprot_id             string_id\n",
       "0     O95866      O95866|G6B_HUMAN  9606.ENSP00000364964\n",
       "1     Q70Z44    Q70Z44|5HT3D_HUMAN  9606.ENSP00000371929\n",
       "2     Q5I7T1    Q5I7T1|AG10B_HUMAN  9606.ENSP00000310120\n",
       "3     O14514    O14514|AGRB1_HUMAN  9606.ENSP00000430945\n",
       "4     Q9H159    Q9H159|CAD19_HUMAN  9606.ENSP00000262150"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntuadmin/anaconda37/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "number of found:  5799\n"
     ]
    }
   ],
   "source": [
    "trainProtein_weights=generate_tripletProt_embeddings(dfTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Test Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of found:  902\n"
     ]
    }
   ],
   "source": [
    "testProtein_weights=generate_tripletProt_embeddings(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testProtein_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array(testProtein_weights)\n",
    "X_train=np.array(trainProtein_weights)\n",
    "y_train=trainLablesOneHot\n",
    "y_test=testLablesOneHot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  computeAbsoluteAccuracyPerLable(Ytrue,Ypred):\n",
    "    countSingle=0\n",
    "    countSingleCorrect=0\n",
    "    countMultiple=0\n",
    "    countMultipleCorrect=0\n",
    "    YpredOneHot=np.zeros(shape=(Ypred.shape))\n",
    "    results=[]\n",
    "    rows=Ytrue.shape[0]\n",
    "    for i in range(rows):\n",
    "        numLables=len(np.nonzero (Ytrue[i])[0])\n",
    "        ind = np.argpartition(Ypred[i], -numLables)[-numLables:]\n",
    "        YpredOneHot[i][ind]=1\n",
    "        if numLables>1:\n",
    "            countMultiple+=1\n",
    "            if not any (np.logical_xor (YpredOneHot[i],Ytrue[i])):\n",
    "                \n",
    "                countMultipleCorrect+=1\n",
    "        else:\n",
    "            countSingle+=1\n",
    "            if  not any (np.logical_xor (YpredOneHot[i],Ytrue[i])):\n",
    "                countSingleCorrect+=1\n",
    "            \n",
    "    for i in range(6):\n",
    "        corrects=len(np.nonzero(np.logical_and(Ytrue[:,i], YpredOneHot[:,i]))[0])\n",
    "        numAllTrues=len(np.nonzero (Ytrue[:,i])[0])\n",
    "        #this class has no sample\n",
    "        if  numAllTrues==0:\n",
    "                continue\n",
    "        results.append(corrects/numAllTrues)\n",
    "    #print('countSingleCorrect',countSingleCorrect)\n",
    "    #print('countSingle',countSingle)\n",
    "    \n",
    "    return results,(countSingleCorrect/countSingle) ,(countMultipleCorrect/countMultiple)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from  model import naive_CNN_classifier\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=1311\n",
    "embedding_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.expand_dims(X_train,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(W_train,Y_train,epochs):\n",
    "    prec_list=[]; reca_list=[]; fscore_list=[] ; fold=0\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=random_seed,shuffle=True)\n",
    "    AccuracyMultiple=0\n",
    "    AccuracySingle=0\n",
    "    all_histories=[]\n",
    "    Y = [np.argmax(y, axis=None, out=None) for y in Y_train]\n",
    "    for train_index, test_index in skf.split(W_train,Y):     \n",
    "        fold+=1\n",
    "        X_train, X_test = W_train[train_index], W_train[test_index] \n",
    "        y_train, y_test = Y_train[train_index], Y_train[test_index]\n",
    "        model = None # Clearing the NN.\n",
    "        model = naive_CNN_classifier(len(lableDict),embedding_size)\n",
    "        erary_stop=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', restore_best_weights=True)\n",
    "        history=model.fit(X_train, y_train, validation_data=(X_test,y_test) ,epochs=epochs, batch_size=8,verbose=1,callbacks=[erary_stop])\n",
    "        all_histories.append(history)\n",
    "        YtestPredicted=model.predict(X_test)\n",
    "        avePrec,AS,AM=computeAbsoluteAccuracyPerLable(y_test, YtestPredicted)\n",
    "        AccuracySingle+=AS\n",
    "        AccuracyMultiple+=AM\n",
    "        prec_list.append(avePrec)\n",
    "    return prec_list,(AccuracySingle/5),(AccuracyMultiple/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4751 samples, validate on 1188 samples\n",
      "Epoch 1/200\n",
      "4751/4751 [==============================] - 3s 651us/step - loss: 0.4706 - val_loss: 0.3941\n",
      "Epoch 2/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.3526 - val_loss: 0.3207\n",
      "Epoch 3/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2995 - val_loss: 0.2876\n",
      "Epoch 4/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2785 - val_loss: 0.2739\n",
      "Epoch 5/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2695 - val_loss: 0.2673\n",
      "Epoch 6/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2645 - val_loss: 0.2630\n",
      "Epoch 7/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2604 - val_loss: 0.2601\n",
      "Epoch 8/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2575 - val_loss: 0.2579\n",
      "Epoch 9/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2550 - val_loss: 0.2554\n",
      "Epoch 10/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2529 - val_loss: 0.2542\n",
      "Epoch 11/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2509 - val_loss: 0.2524\n",
      "Epoch 12/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2491 - val_loss: 0.2510\n",
      "Epoch 13/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2474 - val_loss: 0.2512\n",
      "Epoch 14/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2458 - val_loss: 0.2498\n",
      "Epoch 15/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2445 - val_loss: 0.2480\n",
      "Epoch 16/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2433 - val_loss: 0.2478\n",
      "Epoch 17/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2421 - val_loss: 0.2465\n",
      "Epoch 18/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2412 - val_loss: 0.2459\n",
      "Epoch 19/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2400 - val_loss: 0.2447\n",
      "Epoch 20/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2392 - val_loss: 0.2448\n",
      "Epoch 21/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2380 - val_loss: 0.2438\n",
      "Epoch 22/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2372 - val_loss: 0.2433\n",
      "Epoch 23/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2364 - val_loss: 0.2428\n",
      "Epoch 24/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2356 - val_loss: 0.2440\n",
      "Epoch 25/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2349 - val_loss: 0.2419\n",
      "Epoch 26/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2340 - val_loss: 0.2423\n",
      "Epoch 27/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2332 - val_loss: 0.2412\n",
      "Epoch 28/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2328 - val_loss: 0.2419\n",
      "Epoch 29/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2320 - val_loss: 0.2427\n",
      "Epoch 30/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2314 - val_loss: 0.2403\n",
      "Epoch 31/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2306 - val_loss: 0.2406\n",
      "Epoch 32/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2302 - val_loss: 0.2403\n",
      "Epoch 33/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2295 - val_loss: 0.2402\n",
      "Epoch 34/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2291 - val_loss: 0.2408\n",
      "Epoch 35/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2287 - val_loss: 0.2399\n",
      "Epoch 36/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2275 - val_loss: 0.2390\n",
      "Epoch 37/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2271 - val_loss: 0.2402\n",
      "Epoch 38/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2267 - val_loss: 0.2404\n",
      "Epoch 39/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2264 - val_loss: 0.2391\n",
      "Epoch 40/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2259 - val_loss: 0.2385\n",
      "Epoch 41/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2256 - val_loss: 0.2388\n",
      "Epoch 42/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2250 - val_loss: 0.2386\n",
      "Epoch 43/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2245 - val_loss: 0.2389\n",
      "Epoch 44/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2238 - val_loss: 0.2386\n",
      "Epoch 45/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2232 - val_loss: 0.2382\n",
      "Epoch 46/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2229 - val_loss: 0.2381\n",
      "Epoch 47/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2225 - val_loss: 0.2397\n",
      "Epoch 48/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2225 - val_loss: 0.2381\n",
      "Epoch 49/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2218 - val_loss: 0.2385\n",
      "Epoch 50/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2213 - val_loss: 0.2377\n",
      "Epoch 51/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2209 - val_loss: 0.2391\n",
      "Epoch 52/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2206 - val_loss: 0.2384\n",
      "Epoch 53/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2200 - val_loss: 0.2372\n",
      "Epoch 54/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2197 - val_loss: 0.2379\n",
      "Epoch 55/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2196 - val_loss: 0.2386\n",
      "Epoch 56/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2188 - val_loss: 0.2385\n",
      "Epoch 57/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2186 - val_loss: 0.2385\n",
      "Epoch 58/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2183 - val_loss: 0.2378\n",
      "Train on 4751 samples, validate on 1188 samples\n",
      "Epoch 1/200\n",
      "4751/4751 [==============================] - 3s 652us/step - loss: 0.4866 - val_loss: 0.3905\n",
      "Epoch 2/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.3506 - val_loss: 0.3157\n",
      "Epoch 3/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2963 - val_loss: 0.2833\n",
      "Epoch 4/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2761 - val_loss: 0.2726\n",
      "Epoch 5/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2675 - val_loss: 0.2685\n",
      "Epoch 6/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2626 - val_loss: 0.2653\n",
      "Epoch 7/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2590 - val_loss: 0.2624\n",
      "Epoch 8/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2562 - val_loss: 0.2603\n",
      "Epoch 9/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2537 - val_loss: 0.2588\n",
      "Epoch 10/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2514 - val_loss: 0.2574\n",
      "Epoch 11/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2495 - val_loss: 0.2565\n",
      "Epoch 12/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2477 - val_loss: 0.2544\n",
      "Epoch 13/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2459 - val_loss: 0.2534\n",
      "Epoch 14/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2445 - val_loss: 0.2521\n",
      "Epoch 15/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2430 - val_loss: 0.2511\n",
      "Epoch 16/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2416 - val_loss: 0.2507\n",
      "Epoch 17/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2402 - val_loss: 0.2502\n",
      "Epoch 18/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2391 - val_loss: 0.2487\n",
      "Epoch 19/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2379 - val_loss: 0.2476\n",
      "Epoch 20/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2368 - val_loss: 0.2494\n",
      "Epoch 21/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2360 - val_loss: 0.2468\n",
      "Epoch 22/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2350 - val_loss: 0.2461\n",
      "Epoch 23/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2341 - val_loss: 0.2457\n",
      "Epoch 24/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2333 - val_loss: 0.2456\n",
      "Epoch 25/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2323 - val_loss: 0.2452\n",
      "Epoch 26/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2314 - val_loss: 0.2454\n",
      "Epoch 27/200\n",
      "4751/4751 [==============================] - 1s 272us/step - loss: 0.2309 - val_loss: 0.2453\n",
      "Epoch 28/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2300 - val_loss: 0.2442\n",
      "Epoch 29/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2292 - val_loss: 0.2439\n",
      "Epoch 30/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2287 - val_loss: 0.2434\n",
      "Epoch 31/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2278 - val_loss: 0.2445\n",
      "Epoch 32/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2274 - val_loss: 0.2438\n",
      "Epoch 33/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2267 - val_loss: 0.2427\n",
      "Epoch 34/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2259 - val_loss: 0.2429\n",
      "Epoch 35/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2252 - val_loss: 0.2426\n",
      "Epoch 36/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2247 - val_loss: 0.2427\n",
      "Epoch 37/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2240 - val_loss: 0.2421\n",
      "Epoch 38/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2236 - val_loss: 0.2429\n",
      "Epoch 39/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2230 - val_loss: 0.2420\n",
      "Epoch 40/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2226 - val_loss: 0.2415\n",
      "Epoch 41/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2221 - val_loss: 0.2414\n",
      "Epoch 42/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2217 - val_loss: 0.2416\n",
      "Epoch 43/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2210 - val_loss: 0.2412\n",
      "Epoch 44/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2205 - val_loss: 0.2417\n",
      "Epoch 45/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2200 - val_loss: 0.2409\n",
      "Epoch 46/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2195 - val_loss: 0.2405\n",
      "Epoch 47/200\n",
      "4751/4751 [==============================] - 1s 275us/step - loss: 0.2192 - val_loss: 0.2405\n",
      "Epoch 48/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2185 - val_loss: 0.2411\n",
      "Epoch 49/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2182 - val_loss: 0.2406\n",
      "Epoch 50/200\n",
      "4751/4751 [==============================] - 1s 273us/step - loss: 0.2179 - val_loss: 0.2407\n",
      "Epoch 51/200\n",
      "4751/4751 [==============================] - 1s 274us/step - loss: 0.2169 - val_loss: 0.2419\n",
      "Train on 4751 samples, validate on 1188 samples\n",
      "Epoch 1/200\n",
      "4751/4751 [==============================] - 3s 668us/step - loss: 0.4693 - val_loss: 0.3782\n",
      "Epoch 2/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.3413 - val_loss: 0.3051\n",
      "Epoch 3/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2905 - val_loss: 0.2756\n",
      "Epoch 4/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2724 - val_loss: 0.2664\n",
      "Epoch 5/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2652 - val_loss: 0.2615\n",
      "Epoch 6/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2608 - val_loss: 0.2586\n",
      "Epoch 7/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2576 - val_loss: 0.2570\n",
      "Epoch 8/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2550 - val_loss: 0.2549\n",
      "Epoch 9/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2527 - val_loss: 0.2530\n",
      "Epoch 10/200\n",
      "4751/4751 [==============================] - ETA: 0s - loss: 0.250 - 1s 279us/step - loss: 0.2506 - val_loss: 0.2519\n",
      "Epoch 11/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2487 - val_loss: 0.2506\n",
      "Epoch 12/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2469 - val_loss: 0.2505\n",
      "Epoch 13/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2454 - val_loss: 0.2482\n",
      "Epoch 14/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2439 - val_loss: 0.2477\n",
      "Epoch 15/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2424 - val_loss: 0.2472\n",
      "Epoch 16/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2412 - val_loss: 0.2459\n",
      "Epoch 17/200\n",
      "4751/4751 [==============================] - 1s 282us/step - loss: 0.2399 - val_loss: 0.2451\n",
      "Epoch 18/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2388 - val_loss: 0.2450\n",
      "Epoch 19/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2375 - val_loss: 0.2461\n",
      "Epoch 20/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2368 - val_loss: 0.2437\n",
      "Epoch 21/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2356 - val_loss: 0.2429\n",
      "Epoch 22/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2346 - val_loss: 0.2434\n",
      "Epoch 23/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2338 - val_loss: 0.2428\n",
      "Epoch 24/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2328 - val_loss: 0.2425\n",
      "Epoch 25/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2320 - val_loss: 0.2421\n",
      "Epoch 26/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2315 - val_loss: 0.2410\n",
      "Epoch 27/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2305 - val_loss: 0.2403\n",
      "Epoch 28/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2299 - val_loss: 0.2405\n",
      "Epoch 29/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2292 - val_loss: 0.2401\n",
      "Epoch 30/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2285 - val_loss: 0.2397\n",
      "Epoch 31/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2276 - val_loss: 0.2394\n",
      "Epoch 32/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2272 - val_loss: 0.2395\n",
      "Epoch 33/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2266 - val_loss: 0.2388\n",
      "Epoch 34/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2259 - val_loss: 0.2390\n",
      "Epoch 35/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2253 - val_loss: 0.2384\n",
      "Epoch 36/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2248 - val_loss: 0.2395\n",
      "Epoch 37/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2242 - val_loss: 0.2388\n",
      "Epoch 38/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2237 - val_loss: 0.2388\n",
      "Epoch 39/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2231 - val_loss: 0.2388\n",
      "Epoch 40/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2226 - val_loss: 0.2384\n",
      "Epoch 41/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2217 - val_loss: 0.2384\n",
      "Epoch 42/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2216 - val_loss: 0.2379\n",
      "Epoch 43/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2209 - val_loss: 0.2382\n",
      "Epoch 44/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2207 - val_loss: 0.2371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2202 - val_loss: 0.2380\n",
      "Epoch 46/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2196 - val_loss: 0.2371\n",
      "Epoch 47/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2191 - val_loss: 0.2383\n",
      "Epoch 48/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2189 - val_loss: 0.2374\n",
      "Epoch 49/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2183 - val_loss: 0.2384\n",
      "Epoch 50/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2180 - val_loss: 0.2368\n",
      "Epoch 51/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2175 - val_loss: 0.2372\n",
      "Epoch 52/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2173 - val_loss: 0.2372\n",
      "Epoch 53/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2168 - val_loss: 0.2377\n",
      "Epoch 54/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2165 - val_loss: 0.2380\n",
      "Epoch 55/200\n",
      "4751/4751 [==============================] - 1s 276us/step - loss: 0.2160 - val_loss: 0.2370\n",
      "Train on 4751 samples, validate on 1188 samples\n",
      "Epoch 1/200\n",
      "4751/4751 [==============================] - 3s 678us/step - loss: 0.4831 - val_loss: 0.3950\n",
      "Epoch 2/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.3571 - val_loss: 0.3190\n",
      "Epoch 3/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.3035 - val_loss: 0.2824\n",
      "Epoch 4/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2817 - val_loss: 0.2680\n",
      "Epoch 5/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2722 - val_loss: 0.2616\n",
      "Epoch 6/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2670 - val_loss: 0.2573\n",
      "Epoch 7/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2634 - val_loss: 0.2544\n",
      "Epoch 8/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2605 - val_loss: 0.2523\n",
      "Epoch 9/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2580 - val_loss: 0.2503\n",
      "Epoch 10/200\n",
      "4751/4751 [==============================] - 1s 282us/step - loss: 0.2559 - val_loss: 0.2486\n",
      "Epoch 11/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2539 - val_loss: 0.2476\n",
      "Epoch 12/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2523 - val_loss: 0.2460\n",
      "Epoch 13/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2507 - val_loss: 0.2450\n",
      "Epoch 14/200\n",
      "4751/4751 [==============================] - 1s 281us/step - loss: 0.2492 - val_loss: 0.2437\n",
      "Epoch 15/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2479 - val_loss: 0.2427\n",
      "Epoch 16/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2465 - val_loss: 0.2416\n",
      "Epoch 17/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2453 - val_loss: 0.2411\n",
      "Epoch 18/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2440 - val_loss: 0.2407\n",
      "Epoch 19/200\n",
      "4751/4751 [==============================] - 1s 281us/step - loss: 0.2433 - val_loss: 0.2393\n",
      "Epoch 20/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2421 - val_loss: 0.2385\n",
      "Epoch 21/200\n",
      "4751/4751 [==============================] - 1s 281us/step - loss: 0.2411 - val_loss: 0.2380\n",
      "Epoch 22/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2402 - val_loss: 0.2376\n",
      "Epoch 23/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2393 - val_loss: 0.2370\n",
      "Epoch 24/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2386 - val_loss: 0.2359\n",
      "Epoch 25/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2376 - val_loss: 0.2361\n",
      "Epoch 26/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2370 - val_loss: 0.2349\n",
      "Epoch 27/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2364 - val_loss: 0.2348\n",
      "Epoch 28/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2354 - val_loss: 0.2343\n",
      "Epoch 29/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2348 - val_loss: 0.2338\n",
      "Epoch 30/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2340 - val_loss: 0.2338\n",
      "Epoch 31/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2337 - val_loss: 0.2326\n",
      "Epoch 32/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2326 - val_loss: 0.2336\n",
      "Epoch 33/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2323 - val_loss: 0.2324\n",
      "Epoch 34/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2316 - val_loss: 0.2320\n",
      "Epoch 35/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2311 - val_loss: 0.2317\n",
      "Epoch 36/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2302 - val_loss: 0.2318\n",
      "Epoch 37/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2300 - val_loss: 0.2307\n",
      "Epoch 38/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2296 - val_loss: 0.2304\n",
      "Epoch 39/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2289 - val_loss: 0.2305\n",
      "Epoch 40/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2282 - val_loss: 0.2306\n",
      "Epoch 41/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2275 - val_loss: 0.2312\n",
      "Epoch 42/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2275 - val_loss: 0.2301\n",
      "Epoch 43/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2267 - val_loss: 0.2302\n",
      "Epoch 44/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2263 - val_loss: 0.2298\n",
      "Epoch 45/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2259 - val_loss: 0.2293\n",
      "Epoch 46/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2254 - val_loss: 0.2296\n",
      "Epoch 47/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2248 - val_loss: 0.2290\n",
      "Epoch 48/200\n",
      "4751/4751 [==============================] - 1s 281us/step - loss: 0.2245 - val_loss: 0.2289\n",
      "Epoch 49/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2239 - val_loss: 0.2297\n",
      "Epoch 50/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2237 - val_loss: 0.2286\n",
      "Epoch 51/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2230 - val_loss: 0.2287\n",
      "Epoch 52/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2229 - val_loss: 0.2284\n",
      "Epoch 53/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2223 - val_loss: 0.2284\n",
      "Epoch 54/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2221 - val_loss: 0.2282\n",
      "Epoch 55/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2218 - val_loss: 0.2281\n",
      "Epoch 56/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2211 - val_loss: 0.2284\n",
      "Epoch 57/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2209 - val_loss: 0.2274\n",
      "Epoch 58/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2205 - val_loss: 0.2274\n",
      "Epoch 59/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2200 - val_loss: 0.2278\n",
      "Epoch 60/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2197 - val_loss: 0.2275\n",
      "Epoch 61/200\n",
      "4751/4751 [==============================] - 1s 281us/step - loss: 0.2194 - val_loss: 0.2278\n",
      "Epoch 62/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2191 - val_loss: 0.2278\n",
      "Epoch 63/200\n",
      "4751/4751 [==============================] - 1s 277us/step - loss: 0.2188 - val_loss: 0.2273\n",
      "Epoch 64/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2185 - val_loss: 0.2274\n",
      "Epoch 65/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2181 - val_loss: 0.2270\n",
      "Epoch 66/200\n",
      "4751/4751 [==============================] - 1s 280us/step - loss: 0.2177 - val_loss: 0.2275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2173 - val_loss: 0.2273\n",
      "Epoch 68/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2172 - val_loss: 0.2272\n",
      "Epoch 69/200\n",
      "4751/4751 [==============================] - 1s 278us/step - loss: 0.2169 - val_loss: 0.2270\n",
      "Epoch 70/200\n",
      "4751/4751 [==============================] - 1s 279us/step - loss: 0.2164 - val_loss: 0.2272\n",
      "Train on 4752 samples, validate on 1187 samples\n",
      "Epoch 1/200\n",
      "4752/4752 [==============================] - 3s 685us/step - loss: 0.4746 - val_loss: 0.3883\n",
      "Epoch 2/200\n",
      "4752/4752 [==============================] - 1s 277us/step - loss: 0.3434 - val_loss: 0.3181\n",
      "Epoch 3/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2920 - val_loss: 0.2926\n",
      "Epoch 4/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2737 - val_loss: 0.2842\n",
      "Epoch 5/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2661 - val_loss: 0.2796\n",
      "Epoch 6/200\n",
      "4752/4752 [==============================] - 1s 277us/step - loss: 0.2615 - val_loss: 0.2761\n",
      "Epoch 7/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2581 - val_loss: 0.2736\n",
      "Epoch 8/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2554 - val_loss: 0.2716\n",
      "Epoch 9/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2529 - val_loss: 0.2700\n",
      "Epoch 10/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2508 - val_loss: 0.2688\n",
      "Epoch 11/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2487 - val_loss: 0.2670\n",
      "Epoch 12/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2469 - val_loss: 0.2663\n",
      "Epoch 13/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2455 - val_loss: 0.2641\n",
      "Epoch 14/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2441 - val_loss: 0.2630\n",
      "Epoch 15/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2423 - val_loss: 0.2621\n",
      "Epoch 16/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2411 - val_loss: 0.2614\n",
      "Epoch 17/200\n",
      "4752/4752 [==============================] - 1s 277us/step - loss: 0.2400 - val_loss: 0.2603\n",
      "Epoch 18/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2388 - val_loss: 0.2596\n",
      "Epoch 19/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2377 - val_loss: 0.2588\n",
      "Epoch 20/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2365 - val_loss: 0.2583\n",
      "Epoch 21/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2357 - val_loss: 0.2578\n",
      "Epoch 22/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2346 - val_loss: 0.2575\n",
      "Epoch 23/200\n",
      "4752/4752 [==============================] - 1s 281us/step - loss: 0.2338 - val_loss: 0.2569\n",
      "Epoch 24/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2330 - val_loss: 0.2555\n",
      "Epoch 25/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2321 - val_loss: 0.2552\n",
      "Epoch 26/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2310 - val_loss: 0.2552\n",
      "Epoch 27/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2304 - val_loss: 0.2542\n",
      "Epoch 28/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2300 - val_loss: 0.2537\n",
      "Epoch 29/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2290 - val_loss: 0.2540\n",
      "Epoch 30/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2285 - val_loss: 0.2529\n",
      "Epoch 31/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2277 - val_loss: 0.2528\n",
      "Epoch 32/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2271 - val_loss: 0.2523\n",
      "Epoch 33/200\n",
      "4752/4752 [==============================] - 1s 281us/step - loss: 0.2266 - val_loss: 0.2519\n",
      "Epoch 34/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2261 - val_loss: 0.2516\n",
      "Epoch 35/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2255 - val_loss: 0.2522\n",
      "Epoch 36/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2249 - val_loss: 0.2513\n",
      "Epoch 37/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2242 - val_loss: 0.2512\n",
      "Epoch 38/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2240 - val_loss: 0.2505\n",
      "Epoch 39/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2234 - val_loss: 0.2507\n",
      "Epoch 40/200\n",
      "4752/4752 [==============================] - 1s 281us/step - loss: 0.2228 - val_loss: 0.2510\n",
      "Epoch 41/200\n",
      "4752/4752 [==============================] - 1s 277us/step - loss: 0.2223 - val_loss: 0.2503\n",
      "Epoch 42/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2219 - val_loss: 0.2499\n",
      "Epoch 43/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2212 - val_loss: 0.2500\n",
      "Epoch 44/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2208 - val_loss: 0.2497\n",
      "Epoch 45/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2203 - val_loss: 0.2496\n",
      "Epoch 46/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2201 - val_loss: 0.2488\n",
      "Epoch 47/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2196 - val_loss: 0.2486\n",
      "Epoch 48/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2191 - val_loss: 0.2486\n",
      "Epoch 49/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2185 - val_loss: 0.2485\n",
      "Epoch 50/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2184 - val_loss: 0.2487\n",
      "Epoch 51/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2176 - val_loss: 0.2484\n",
      "Epoch 52/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2173 - val_loss: 0.2477\n",
      "Epoch 53/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2170 - val_loss: 0.2480\n",
      "Epoch 54/200\n",
      "4752/4752 [==============================] - 1s 280us/step - loss: 0.2166 - val_loss: 0.2487\n",
      "Epoch 55/200\n",
      "4752/4752 [==============================] - 1s 279us/step - loss: 0.2164 - val_loss: 0.2482\n",
      "Epoch 56/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2158 - val_loss: 0.2479\n",
      "Epoch 57/200\n",
      "4752/4752 [==============================] - 1s 278us/step - loss: 0.2156 - val_loss: 0.2484\n"
     ]
    }
   ],
   "source": [
    "results,AccuracySingle,AccuracyMultiple=train_and_evaluate_model(X_train,y_train,200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cell_membrane': 0,\n",
       " 'Cytoplasm': 1,\n",
       " 'ER_Golgi': 2,\n",
       " 'Mitochondrion': 3,\n",
       " 'Nucleus': 4,\n",
       " 'Secreted': 5}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lableDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78321365, 0.765912  , 0.65866759, 0.77061711, 0.87988803,\n",
       "       0.73828593])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean (results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average :  0.7660973841940704\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Average : ',np.mean (results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indipndnt Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None # Clearing the NN.\n",
    "\n",
    "model=naive_CNN_classifier(len(lableDict))\n",
    "\n",
    "X_test=np.expand_dims(X_test,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train, y_train, validation_data=(X_test,y_test) ,epochs=30, batch_size=16,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "YtestPredicted=model.predict(X_test)\n",
    "\n",
    "results,AccuracySingle,AccuracyMultiple=computeAbsoluteAccuracyPerLable(y_test, YtestPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7751937984496124,\n",
       " 0.8277027027027027,\n",
       " 0.5422222222222223,\n",
       " 0.5404040404040404,\n",
       " 0.7058823529411765,\n",
       " 0.5535714285714286]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average :  0.6574960908818639\n"
     ]
    }
   ],
   "source": [
    "print('Average : ',np.mean (results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
